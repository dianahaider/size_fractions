{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for importing, formatting and data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#for plotting\n",
    "import matplotlib, random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from upsetplot import plot\n",
    "import plotly.graph_objects as go\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and format metadata from lab, and BBMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lab metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the absorbance per sample data\n",
    "a260230 = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/ch2/bb_data/2022/a260230.csv\")\n",
    "a260230 = a260230.dropna(how='all') #drop null rows and columns\n",
    "a260230.dropna(how='all', axis=1, inplace=True)\n",
    "a260230 = a260230.replace({pd.NA: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload metadata of non size fractionated samples\n",
    "noSF = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/ch2/bb_data/2022/noSF.csv\")\n",
    "noSF = noSF.dropna(how='all')\n",
    "noSF.dropna(how='all', axis=1, inplace=True)\n",
    "noSF = noSF.replace({pd.NA: np.nan})\n",
    "noSF = noSF.dropna(how='all') #drop null rows and columns\n",
    "#uncomment the line below to remove metadata columns\n",
    "#noSF = noSF[[\"sampleid\", \"[DNA]ng/ul\", \"A260/280\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload metadata of size fractionated samples\n",
    "SF = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/ch2/bb_data/2022/SF.csv\")\n",
    "SF = SF.dropna(how='all')\n",
    "SF.dropna(how='all', axis=1, inplace=True)\n",
    "SF = SF.replace({pd.NA: np.nan})\n",
    "SF = SF.rename(columns={'Depth Code 1-A, 5-B, 10-C, 60-D': 'depth_code',\n",
    "                            'Size Code 3um - L 0.2um - S': 'size_code'}) \n",
    "#uncomment the line below to remove metadata columns\n",
    "#SF = SF[[\"sampleid\", \"[DNA]ng/ul\", \"A260/280\", \"date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renumber dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary for months\n",
    "month_dic = {\n",
    "    \"Jan\": 1,\n",
    "    \"Feb\": 2,\n",
    "    \"Mar\": 3,\n",
    "    \"Apr\": 4,\n",
    "    \"May\": 5,\n",
    "    \"Jun\": 6,\n",
    "    \"Jul\": 7,\n",
    "    \"Aug\": 8,\n",
    "    \"Sep\": 9,\n",
    "    \"Oct\": 10,\n",
    "    \"Nov\": 11,\n",
    "    \"Dec\": 12\n",
    "}\n",
    "month_season = {\n",
    "    \"Jan\": \"Winter\",\n",
    "    \"Feb\": \"Winter\",\n",
    "    \"Mar\": \"Spring\",\n",
    "    \"Apr\": \"Spring\",\n",
    "    \"May\": \"Spring\",\n",
    "    \"Jun\": \"Summer\",\n",
    "    \"Jul\": \"Summer\",\n",
    "    \"Aug\": \"Summer\",\n",
    "    \"Sep\": \"Autumn\",\n",
    "    \"Oct\": \"Autumn\",\n",
    "    \"Nov\": \"Autumn\",\n",
    "    \"Dec\": \"Winter\"\n",
    "}\n",
    "depth_num = {\n",
    "    \"A\": 1,\n",
    "    \"B\": 5,\n",
    "    \"C\": 10,\n",
    "    \"D\": 60,\n",
    "    \"E\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dates(md):\n",
    "    if 'weekn' not in md:\n",
    "        md[\"weekn\"] = md[\"sampleid\"].str.extract(r'\\.([1-9][0-9]?)[A-E]')\n",
    "    md['weekn'] = pd.to_numeric(md['weekn'])\n",
    "    md['date'] = md.groupby(['sampleid','weekn'], sort=False, group_keys=False)['date'].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "    #add month to a new column\n",
    "    md['month_name'] = md['date'].str.split('-').str[1]\n",
    "    md['year'] = 2022\n",
    "    md=md[md.year==2022]\n",
    "\n",
    "    #add month number\n",
    "    md['month']= md['month_name'].map(month_dic)\n",
    "\n",
    "    #add day number\n",
    "    md['day'] = md['date'].str.split('-').str[0]\n",
    "    md[[\"year\", \"month\", \"day\"]] = md[[\"year\", \"month\", \"day\"]].apply(pd.to_numeric)\n",
    "\n",
    "    #remove symbol for better handling of data\n",
    "    #md.rename(columns={\"Week#\": \"Weekn\"}, inplace=True)\n",
    "    #md.rename(columns={\"Depth\": \"depth\"}, inplace=True) #to match dfo\n",
    "\n",
    "    #change to int to remove decimals from date columns\n",
    "    md.year = md.year.apply(int)\n",
    "    md.day = md.day.apply(int)\n",
    "    md.month = md.month.apply(int)\n",
    "    #md.depth = md.depth.apply(int)\n",
    "    #md.weekn = md.weekn.apply(int)\n",
    "\n",
    "    #change to str to aggregate them into time_string to match dfos formatting of the date\n",
    "    md.year = md.year.apply(str)\n",
    "    md.month = md.month.apply(str)\n",
    "    md.day = md.day.apply(str)\n",
    "\n",
    "    md[\"depth_code\"] = md[\"sampleid\"].str.extract(r'[1-9][0-9]?([A-E])')\n",
    "    md['depth']= md['depth_code'].map(depth_num)\n",
    "    md['depth'] = pd.to_numeric(md['depth'])\n",
    "\n",
    "    #add leading zero to match date format in dfo metadata\n",
    "    md['month'] = md['month'].str.zfill(2)\n",
    "    md['day'] = md['day'].str.zfill(2)\n",
    "\n",
    "    md['time_string'] = md[['year', 'month', 'day']].agg('-'.join, axis=1)\n",
    "    \n",
    "    md[\"size_code\"] = md[\"sampleid\"].str.extract(r'[1-9][0-9]?[A-E]([L-S])')\n",
    "    md[\"size_code\"] = md[\"size_code\"].fillna('W')\n",
    "    \n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = fill_dates(SF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noSF = noSF[noSF['weekn'] < 17]\n",
    "noSF = fill_dates(noSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify which columns are shared between two dataframes\n",
    "a = np.intersect1d(SF.columns, noSF.columns)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsnum = ['A260/280', '[DNA]ng/ul', 'depth', 'elution_volume', 'filtration_volume ', 'weekn', 'year']\n",
    "noSF[colsnum] = noSF[colsnum].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "SF[colsnum] = SF[colsnum].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "colstr = ['sampleid']\n",
    "SF[colstr] = SF[colstr].astype(\"string\")\n",
    "\n",
    "SF = SF.replace({pd.NA: np.nan})\n",
    "noSF = noSF.replace({pd.NA: np.nan})\n",
    "\n",
    "mdsf = noSF.merge(SF, on=['A260/280', 'Extracted_By', 'Notes', '[DNA]ng/ul', 'date', 'day',\n",
    "                           'depth', 'depth_code', 'elution_volume', 'extraction_date',\n",
    "                           'filtration_volume ', 'month', 'month_name', 'sampleid',\n",
    "                           'size_code', 'time_string', 'weekn', 'year'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.intersect1d(mdsf.columns, a260230.columns)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing cell values with matching column name from other dataframe\n",
    "\n",
    "a260230[\"weekn\"] = a260230[\"sampleid\"].str.extract(r'\\.([1-9][0-9]?)[A-E]')\n",
    "a260230['weekn'] = pd.to_numeric(a260230['weekn'])\n",
    "    \n",
    "a260230 = a260230.fillna(noSF)\n",
    "mdsf = mdsf.fillna(a260230)\n",
    "\n",
    "a260230[\"depth_code\"] = a260230[\"sampleid\"].str.extract(r'[1-9][0-9]?([A-E])')\n",
    "a260230['depth']= a260230['depth_code'].map(depth_num)\n",
    "a260230['depth'] = pd.to_numeric(a260230['depth'])\n",
    "\n",
    "\n",
    "mdsf2 = mdsf.merge(a260230, on=['A260/280', '[DNA]ng/ul', 'extraction_date', 'sampleid', 'weekn', 'depth_code','depth'], how='outer')\n",
    "\n",
    "mdsf[\"weekn\"] = mdsf[\"sampleid\"].str.extract(r'\\.([1-9][0-9]?)[A-E]')\n",
    "mdsf['weekn'] = pd.to_numeric(mdsf['weekn'])\n",
    "mdsf['date'] = mdsf.groupby(['weekn'], sort=False)['date'].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "mdsf = mdsf[mdsf['weekn'] < 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = mdsf2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.sort_values(by=['weekn', 'depth'],inplace=True)\n",
    "md = md.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.to_csv('metadata_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and manage BBMP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata __md__ is formatted. It contains 38 columns.\n",
    "__md__ is the lab's metadata for sampling, extraction and sequencing. \\\n",
    "__dfo_md__ is BBMP remote sensing data (salinity, pH, temperature, density..) \\\n",
    "__bio_niskin__ is nutrient data \\\n",
    "Format __bio_niskin__ data to merge with __md__. __bio_niskin__ is 32 columns, including year, month, day, and depth. __dfo_md__ also has 32 columns, including year_time, month_time, day_time. To merge these data with __md__, we will change the time stamps columns to the same name, and generate a time_string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_md = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/ch2/bb_data/bbmp_aggregated_profiles.csv\")\n",
    "bio_niskin = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/ch2/bb_data/BBMP_Data_2022.csv\")#\n",
    "#dfo_metadata_y14 = pd.read_csv(\"/Users/Diana/Documents/escuela/phd/bb_data/2019/data_export/trim-analysis/dfo_metadata_y14.tsv\", sep='\\t')\n",
    "\n",
    "#change to str to aggregate them into time_string\n",
    "bio_niskin = bio_niskin[bio_niskin.year==2022]\n",
    "bio_niskin.year = bio_niskin.year.apply(str)\n",
    "bio_niskin.month = bio_niskin.month.apply(str)\n",
    "bio_niskin.day = bio_niskin.day.apply(str)\n",
    "#add leading zero to match date format in dfo metadata\n",
    "bio_niskin['month'] = bio_niskin['month'].str.zfill(2)\n",
    "bio_niskin['day'] = bio_niskin['day'].str.zfill(2)\n",
    "\n",
    "bio_niskin['time_string'] = bio_niskin[['year', 'month', 'day']].agg('-'.join, axis=1)\n",
    "\n",
    "#make a new column for time_string without the time\n",
    "dfo_md=dfo_md[dfo_md.year_time==2022]\n",
    "dfo_md['time_string_time'] = dfo_md['time_string']\n",
    "dfo_md['time_string'] = dfo_md['time_string'].str.split(' ').str[0]\n",
    "\n",
    "#renaming columns to ensure correct merging\n",
    "dfo_md.rename(columns={\"depth\":\"bbmpdepth\",\"pressure\": \"depth\", \"year_time\": \"year\", \"month_time\": \"month\", \"day_time\": \"day\"}, inplace=True)\n",
    "\n",
    "#change to int to remove decimals from date columns\n",
    "cols = ['year', 'depth', 'month', 'day']\n",
    "md[cols] = md[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "dfo_md[cols] = dfo_md[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "bio_niskin[cols] = bio_niskin[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "\n",
    "#drop empty columns and rows\n",
    "dfo_md.dropna(how='all', axis=1, inplace=True) #empty cols\n",
    "dfo_md.dropna(how='all', inplace=True) #empty rows\n",
    "\n",
    "bio_niskin.dropna(how='all', axis=1, inplace=True) #empty cols\n",
    "bio_niskin.dropna(how='all', inplace=True) #empty rows\n",
    "\n",
    "#make a season column\n",
    "md['season'] = ''\n",
    "\n",
    "for month, season in month_season.items():\n",
    "    md.loc[md['month_name'] == month, 'season'] = season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bio_niskin data has exact recorded depths, whereas BB sample data is restricted to categories: make a new column to allow for data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.array([1,5,10,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_niskin2= bio_niskin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 48 #number of weeks for the tile repeat\n",
    "bio_niskin2['NewDepth'] = pd.DataFrame({'NewDepth': np.tile(depths, length)}) #tile depth categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_niskin2=bio_niskin2.assign(NewDepth=depths[np.arange(len(bio_niskin2)) % len(depths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the two depth columns at the end of the dataframe to visually examine\n",
    "cols_at_end = ['depth', 'NewDepth']\n",
    "bio_niskin3 = bio_niskin2[[c for c in bio_niskin2 if c not in cols_at_end] \n",
    "        + [c for c in cols_at_end if c in bio_niskin2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to ensure correct merging\n",
    "bio_niskin3.rename(columns={'depth': 'truedepth', 'NewDepth': 'depth'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make merging columns to same type\n",
    "bio_niskin3[cols] = bio_niskin3[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "md[cols] = md[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "dfo_md[cols] = md[cols].apply(pd.to_numeric, errors='ignore', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.intersect1d(md.columns, dfo_md.columns)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview column types to allow for merging\n",
    "#pd.set_option('display.max_rows', 35)\n",
    "#md.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert merging columns to same type\n",
    "colsnum = ['day', 'month']\n",
    "dfo_md[colsnum] = dfo_md[colsnum].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "md[colsnum] = md[colsnum].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging party\n",
    "merged = pd.merge(md, dfo_md, on=['day', 'depth', 'month', 'pH', 'sigmaTheta', 'theta',\n",
    "                                  'time_string', 'year'], how=\"left\")\n",
    "allyears = pd.merge(md, dfo_md, on=['day', 'depth', 'month', 'pH', 'sigmaTheta', 'theta',\n",
    "                                    'time_string', 'year'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged['weekn'] < 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('metadata_niskin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert column type to numeric for merging\n",
    "allyears[cols] = allyears[cols].apply(pd.to_numeric, errors='ignore', axis=1)\n",
    "\n",
    "#merged = merged.drop(index=237) #delete a row with missing information\n",
    "merged[cols] = merged[cols].apply(pd.to_numeric, axis=1)\n",
    "bio_niskin3[cols] = bio_niskin3[cols].apply(pd.to_numeric, axis=1)\n",
    "\n",
    "#add nutrient data\n",
    "#uncomment the line below if  need access to metadata outside the 16weeks samples in 2022\n",
    "#preall_md= pd.merge(allyears, bio_niskin3, on=[\"day\", \"month\", \"year\", 'depth'], how=\"outer\")\n",
    "all_md = pd.merge(merged, bio_niskin3, on=[\"day\", \"month\", \"year\", 'depth'], how=\"left\")\n",
    "\n",
    "#split dfs by depth\n",
    "shallow_depths = [1, 5, 10]\n",
    "shallow = all_md[all_md[\"depth\"] < 30]\n",
    "#shallow = shallow.groupby(['year', 'month', \"day\"]).mean().reset_index()\n",
    "deep = all_md[all_md.depth == 60]\n",
    "\n",
    "#split dfs by season\n",
    "year_season = all_md.groupby(by = ['year','season']).mean().reset_index()\n",
    "\n",
    "Winter = year_season.loc[year_season['season'] == 'Winter',:]\n",
    "Spring = year_season.loc[year_season['season'] == 'Spring',:]\n",
    "Summer = year_season.loc[year_season['season'] == 'Summer',:]\n",
    "Autumn = year_season.loc[year_season['season'] == 'Autumn',:]\n",
    "\n",
    "#save output as csv\n",
    "all_md.to_csv('allmetadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = all_md[all_md.depth_code == 'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=d1, x=\"weekn\", y=\"Chlorophyll A\", color=\"0.8\", linewidth=.75, kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=d1, x=\"weekn\", y=\"Phosphate\", color=\"0.8\", linewidth=.75, kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find rows with null values at given column\n",
    "emptynit = merged[merged['depth'].isna()]\n",
    "emptynit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptynit = merged[merged['temperature'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptynit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly seasonal averages figure\n",
    "fig2 = go.Figure()\n",
    "for template in [\"plotly_white\"]:\n",
    "    fig2.add_trace(go.Scatter(x=Winter['year'], y=Winter['temperature'],\n",
    "                    mode='lines',\n",
    "                    name='Winter',\n",
    "                    marker_color='#838B8B'))\n",
    "    fig2.add_trace(go.Scatter(x=Spring['year'], y=Spring['temperature'],\n",
    "                    mode='lines',\n",
    "                    name='Spring',\n",
    "                    marker_color='#FFB5C5'))\n",
    "    fig2.add_trace(go.Scatter(x=Summer['year'], y=Summer['temperature'],\n",
    "                    mode='lines',\n",
    "                    name='Summer',\n",
    "                    marker_color='#87CEFF'))\n",
    "    fig2.add_trace(go.Scatter(x=Autumn['year'], y=Autumn['temperature'],\n",
    "                    mode='lines',\n",
    "                    name='Autumn',\n",
    "                    marker_color='#FF8000'))\n",
    "    fig2.update_layout(\n",
    "    height=800,\n",
    "    xaxis_title=\"Years\",\n",
    "    yaxis_title='Temperature in degree',\n",
    "    title_text='Average Temperature seasonwise over the years',\n",
    "    template=template)\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn season averages plot\n",
    "sns.lineplot(year_season['year'],year_season['temperature'], hue =year_season[\"season\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and plot anomalies in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(metadata, df, dpt, yr=all, month=all):\n",
    "    \n",
    "    sfd=df[df.depth==dpt]\n",
    "    \n",
    "    md_col = sfd[['event_id', metadata, \"year\", \"month\"]].copy()\n",
    "    md_col = md_col[md_col[metadata].notna()]\n",
    "    if yr != all:\n",
    "        #mdcol_yr = md_col[md_col.Year == yr]\n",
    "        mdcol_yr = md_col[md_col['year'].isin(yr)]\n",
    "    else: \n",
    "        mdcol_yr = md_col\n",
    "        \n",
    "    if month != all:\n",
    "        #mdcol_yr = mdcol_yr[mdcol_yr.Month == month]\n",
    "        mdcol_yr = mdcol_yr[mdcol_yr['month'].isin(month)]\n",
    "    \n",
    "    mdcol_yr = mdcol_yr.drop(columns=['year', \"month\"])\n",
    "    mdcol_yr = mdcol_yr.set_index(['event_id'])\n",
    "    \n",
    "    #modelling time\n",
    "    outliers_fraction = float(.01)\n",
    "    scaler = StandardScaler()\n",
    "    np_scaled = scaler.fit_transform(mdcol_yr.values.reshape(-1, 1))\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "    # train isolation forest\n",
    "    model =  IsolationForest(contamination=outliers_fraction)\n",
    "    model.fit(data)\n",
    "    \n",
    "    #predict data\n",
    "    mdcol_yr['anomaly'] = model.predict(data)\n",
    "    \n",
    "    # visualization\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    a = mdcol_yr.loc[mdcol_yr['anomaly'] == -1, [metadata]] #anomaly\n",
    "    ax.plot(mdcol_yr.index, mdcol_yr[metadata], color='black', label = 'Normal')\n",
    "    ax.scatter(a.index,a[metadata], color='red', label = 'Anomaly')\n",
    "    #plt.axvline(36, ls='--')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    #add axes names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_anomalies('Temperature', bio_niskin3, 1, yr={2022}, month={1,2,3,4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_anomalies('Temperature', bio_niskin3, 5, yr={2022}, month={1,2,3,4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_anomalies('Temperature', bio_niskin3, 10, yr={2022}, month={1,2,3,4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_anomalies('Temperature', bio_niskin3, 60, yr={2022}, month={1,2,3,4})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-2023.5",
   "language": "python",
   "name": "qiime2-2023.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
