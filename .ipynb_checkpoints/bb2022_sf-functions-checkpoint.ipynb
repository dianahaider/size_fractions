{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special thanks to Alex Manuele https://github.com/alexmanuele\n",
    "def consolidate_tables(MG):\n",
    "    if MG == '16S':\n",
    "        comm = '02-PROKs'\n",
    "    else :\n",
    "        comm = '02-EUKs'\n",
    "        \n",
    "    table_list = glob.glob('{0}/table.qza'.format('/Users/Diana/Documents/escuela/phd/size_fractions/BB22_size-fraction-comparison-analysed/to_transfer/'+comm))\n",
    "    print(\"Found all \"+MG+\" tables.\")\n",
    "\n",
    "        \n",
    "    dataframes = []  \n",
    "    for table_path in table_list:\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            #load table, dump contents to tempdir\n",
    "            table = Artifact.load(table_path)\n",
    "            #Make sure the tables are all FeatureFrequency type\n",
    "            assert str(table.type) == 'FeatureTable[Frequency]', \"{0}: Expected FeatureTable[Frequency], got {1}\".format(table_path, table.type)\n",
    "            Artifact.extract(table_path, tempdir)\n",
    "            #get the provenance form the tempdir and format it for DF\n",
    "            prov = '{0}/{1}/provenance/'.format(tempdir, table.uuid)\n",
    "            action = yaml.load(open(\"{0}action/action.yaml\".format(prov), 'r'), Loader=yaml.BaseLoader)\n",
    "            paramlist = action['action']['parameters']\n",
    "            paramlist.append({'table_uuid': \"{}\".format(table.uuid)})\n",
    "            paramdict = {}\n",
    "            for record in paramlist:\n",
    "                paramdict.update(record)\n",
    "\n",
    "            # Get the data into a dataframe\n",
    "              #Biom data\n",
    "            df = table.view(pd.DataFrame).unstack().reset_index()\n",
    "            df.columns = ['feature_id', 'sample_name', 'feature_frequency']\n",
    "            df['table_uuid'] = [\"{}\".format(table.uuid)] * df.shape[0]\n",
    "              #param data\n",
    "            pdf = pd.DataFrame.from_records([paramdict])\n",
    "              #merge params into main df\n",
    "            df = df.merge(pdf, on='table_uuid')\n",
    "            \n",
    "\n",
    "            #I like having these columns as the last three. Makes it more readable\n",
    "            cols = df.columns.tolist()\n",
    "            reorder = ['sample_name', 'feature_id', 'feature_frequency']\n",
    "            for val in reorder:\n",
    "                cols.append(cols.pop(cols.index(val)))\n",
    "            df = df[cols]\n",
    "            df['table_path'] = [table_path] * df.shape[0]\n",
    "            df['sample_name'] = df['sample_name'].str.replace('-', '.')\n",
    "            dataframes.append(df)\n",
    "            \n",
    "            # Adding table_id, forward and reverse trim columns\n",
    "            #df['table_id'] = str(table_path.split('/')[-3]) #add a table_id column\n",
    "            #df['forward_trim'], df['reverse_trim'] = df['table_id'].str.split('R', 1).str\n",
    "            #df['forward_trim'] = df['forward_trim'].map(lambda x: x.lstrip('F'))\n",
    "            #df[\"forward_trim\"] = pd.to_numeric(df[\"forward_trim\"])\n",
    "            #df[\"reverse_trim\"] = pd.to_numeric(df[\"reverse_trim\"])\n",
    "\n",
    "    #Stick all the dataframes together\n",
    "    #outputfile=\"merged_all_tables.tsv\"\n",
    "    df = pd.concat(dataframes)\n",
    "    df['sample_name'] = df['sample_name'].str.replace(r'\\.S([1-9]|[1-9][0-9]|[1-9][0-9][0-9]).L001\\.','', regex=True)\n",
    "    \n",
    "    #df.to_csv(comm+'/merged_all_tables.tsv', sep='\\t', index=False)\n",
    "    print(\"Successfully saved all tables.\")\n",
    "    return df, MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata(df):\n",
    "    #df = pd.read_csv('02-PROKs/'+'/merged_all_tables.tsv', sep='\\t')\n",
    "\n",
    "    tables = df[['sample_name', 'feature_id', 'feature_frequency']].copy()\n",
    "    tables.rename(columns={'sample_name':'sampleid'}, inplace=True)\n",
    "\n",
    "    all_md['sampleid'] = all_md['sampleid'].str.replace('_', '.')\n",
    "    merged = pd.merge(tables,all_md, on='sampleid', how='left') #all_md is the metadata file\n",
    "    merged = merged[merged.feature_frequency != 0]\n",
    "    \n",
    "    merged['year'] = 2022\n",
    "\n",
    "    merged[\"size_code\"] = merged[\"sampleid\"].str.extract(r'[1-9][0-9]?[A-E]([L-S])')\n",
    "    merged[\"size_code\"] = merged[\"size_code\"].fillna('W')\n",
    "    merged[\"depth_code\"] = merged[\"sampleid\"].str.extract(r'[1-9][0-9]?([A-E])')\n",
    "    merged['depth']= merged['depth_code'].map(depth_num)\n",
    "    merged[\"weekn\"] = merged[\"sampleid\"].str.extract(r'\\.([1-9][0-9]?)[A-E]')\n",
    "    merged['weekn'] = pd.to_numeric(merged['weekn'])\n",
    "    merged['depth'] = pd.to_numeric(merged['depth'])\n",
    "    merged['date'] = merged.groupby('weekn', as_index=False)['date'].transform('first')\n",
    "    \n",
    "    merged['Total'] = merged['feature_frequency'].groupby(merged['sampleid']).transform('sum')\n",
    "    merged['ratio'] = merged['feature_frequency']/merged['Total']\n",
    "    merged['nASVs'] = merged['feature_id'].groupby(merged['sampleid']).transform('count')\n",
    "    merged['weekdepth'] = merged[\"weekn\"].astype(str) + merged[\"depth\"].astype(str)\n",
    "    merged['avg'] = merged['nASVs'].groupby(merged['weekdepth']).transform('mean')\n",
    "    merged['diff'] = merged['nASVs'] - merged['avg']\n",
    "\n",
    "    print('Set up metadata ...')\n",
    "    \n",
    "    #merged.to_csv(comm+'/merged_asvs_metadata.tsv', sep = '\\t')\n",
    "    print('Saved merged_asvs_metadata.tsv')\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_metadata(merged, depth='all', size_fraction='both', year='all', R='all', F='all', txsubset = 'all'):\n",
    "#make df of features/composition+run+comm\n",
    "\n",
    "    depth = depth\n",
    "    year = year\n",
    "    size_fraction = size_fraction\n",
    "    txsubset = txsubset\n",
    "        \n",
    "    files = glob.glob('{0}/*/class/*/data/taxonomy.tsv'.format('/Users/Diana/Documents/escuela/phd/size_fractions/BB22_size-fraction-comparison-analysed/to_transfer'))\n",
    "    taxos = []\n",
    "#    if not os.path.exists(path+composition):\n",
    "#        os.mkdir(path+composition)\n",
    "    for filename in files:\n",
    "        tax = pd.read_csv(filename, sep='\\t')\n",
    "        taxos.append(tax)\n",
    "        \n",
    "    print('Appended all taxonomies to taxos')\n",
    "    taxos = pd.concat(taxos)\n",
    "    taxos = taxos.rename(columns={\"Feature ID\": \"feature_id\"}, errors=\"raise\")\n",
    "    taxos = taxos.drop_duplicates()\n",
    "\n",
    "    separated = merged.merge(taxos, how='left', on='feature_id') #merged excludes features of frequency = 0\n",
    "    separated = separated.drop_duplicates()\n",
    "    \n",
    "    if depth != 'all':\n",
    "        separated = separated[separated[\"depth\"] == depth]\n",
    "    if size_fraction != 'both':\n",
    "        separated = separated[separated[\"size_fraction\"] == size_fraction]\n",
    "\n",
    "    separated[['Domain', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']] = separated['Taxon'].str.split('; ', expand=True)\n",
    "    cols = ['Domain', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
    "    for col in cols:\n",
    "        separated[col] = separated[col].fillna('Unassigned')\n",
    "        \n",
    "    separated['Month'] = separated['date'].str.split('-').str[1]\n",
    "    \n",
    "    #separated['total'] = separated.groupby(['table_id','sample-id'])['feature_frequency'].transform('sum')\n",
    "    #separated['ratio'] = separated['feature_frequency']/(separated['total'])\n",
    "    #separated_taxonomies = separated.copy()\n",
    "    \n",
    "    #make a dictionary with keys for id-ing the taxon belonging to this sub-community\n",
    "    #separated_dic = pd.Series(separated.Taxon.values,separated.feature_id.values).to_dict()\n",
    "    print('Saved separated by metadata dataframe.')\n",
    "    \n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxbarplot(separated, level, depth, topn): #separated is the df, #level is a string of taxonomic level column name, depth is an integer\n",
    "    sfd=separated[separated.depth==depth]\n",
    "    toptaxa = sfd[['feature_id', 'feature_frequency', 'Taxon', 'size_code', 'depth','weekn', level]].copy()\n",
    "    toptaxa = toptaxa.drop_duplicates()\n",
    "    df_agg = toptaxa.groupby(['size_code',level, 'depth']).agg({'feature_frequency':sum})\n",
    "    topd = df_agg['feature_frequency'].groupby('size_code', group_keys=False).nlargest(topn)\n",
    "    topd = topd.to_frame()\n",
    "    topd = topd.reset_index()\n",
    "\n",
    "\n",
    "    df_agg = df_agg.reset_index()\n",
    "    df_agg['set_name'] = df_agg['size_code']+df_agg['depth'].astype(str)\n",
    "    \n",
    "    cumulab = separated[['feature_frequency', 'depth', 'size_code', 'Genus']].copy()\n",
    "    cumulab1 = cumulab.groupby(['Genus']).agg({'feature_frequency':sum})\n",
    "\n",
    "    resultpivot = df_agg.pivot_table(index=level, columns='set_name', values='feature_frequency')\n",
    "    resultpivot = resultpivot.fillna(0)\n",
    "    resultpivot[resultpivot != 0] = 1\n",
    "    tosave = pd.merge(resultpivot, cumulab1, left_index=True, right_index=True)\n",
    "    tosave.to_csv(level+'_'+str(depth)+'16S_relab.csv')\n",
    "    \n",
    "    top10d_list = topd[level].unique()\n",
    "    top10d = sfd.copy()\n",
    "    top10d.loc[~top10d[level].isin(top10d_list), level] = 'Other' #isnot in top list\n",
    "    phyld = top10d.groupby(['size_code','weekn', level])['ratio'].sum()\n",
    "    phyld = phyld.reset_index()\n",
    "\n",
    "\n",
    "    fig = px.bar(phyld, x=\"size_code\", y=\"ratio\", facet_col=\"weekn\", color=level, labels={\n",
    "                     \"feature_frequency\": \"Relative abundance\",\n",
    "                     \"size_code\": \"\",\n",
    "                     \"weekn\": \"w\"}, color_discrete_map=palette_dict)\n",
    "    fig.update_xaxes(type='category', dtick=1)\n",
    "    fig.update_layout(\n",
    "        title=\"Relative abundance of top 10\" + level + 'observed at Depth' + str(depth),\n",
    "        yaxis_title=\"Relative abundance\",\n",
    "        xaxis_title=\"Size fraction\",\n",
    "        legend_title=level,\n",
    "        font=dict(size=8)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    #fig.write_image(\"outputs/fig1.png\")\n",
    "    #fig.to_image(format=\"png\")\n",
    "    \n",
    "    return phyld, top10d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaplot(separated, depth, comm, columnperm, spc):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        folder = '02-PROKs'\n",
    "    else:\n",
    "        folder = '02-EUKs'\n",
    "        \n",
    "    \n",
    "    if depth == 'all':\n",
    "        df = separated.copy()\n",
    "    else:\n",
    "        df=separated[separated.depth==depth]\n",
    "        \n",
    "    \n",
    "    if 'SL' in separated['size_code'].unique():\n",
    "        #sizecode palette codes\n",
    "        sizecodes = ['S', 'L', 'W', 'SL']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "        dicsc = pd.Series(df.size_code.values,index=df.sampleid).to_dict()\n",
    "        color_rows_sc = {k: palette_dict[v] for k, v in dicsc.items()}\n",
    "        seriescr = pd.Series(color_rows_sc)\n",
    "    \n",
    "    else:\n",
    "        #sizecode palette codes\n",
    "        sizecodes = ['S', 'L', 'W']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "        dicsc = pd.Series(df.size_code.values,index=df.sampleid).to_dict()\n",
    "        color_rows_sc = {k: palette_dict[v] for k, v in dicsc.items()}\n",
    "        seriescr = pd.Series(color_rows_sc)\n",
    "    \n",
    "    #month palette code\n",
    "    df['Month'] = df['date'].str.split('-').str[1]\n",
    "    months = ['Jan', 'Feb', 'Mar', 'May', 'Apr']\n",
    "    palette_colors = sns.color_palette(\"flare\")\n",
    "    palette_dict_month = {monthname: color for monthname, color in zip(months, palette_colors)}\n",
    "    dic = pd.Series(df.Month.values,index=df.sampleid).to_dict()\n",
    "    color_rows_month = {k: palette_dict_month[v] for k, v in dic.items()}\n",
    "    seriesmonthcr = pd.Series(color_rows_month)\n",
    "\n",
    "    dfcolors = pd.DataFrame({'Month': seriesmonthcr,'Size code':seriescr})\n",
    "    \n",
    "    topiv = df[['feature_id', 'feature_frequency', 'sampleid']].copy()\n",
    "    topiv = topiv.drop_duplicates()\n",
    "    \n",
    "    sfdpiv= topiv.pivot(index='sampleid', columns='feature_id', values='feature_frequency')\n",
    "    sfdpiv=sfdpiv.fillna(0)\n",
    "    sfdclr=sfdpiv.mask(sfdpiv==0).fillna(0.1)\n",
    "    clr_transformed_array = clr(sfdclr)\n",
    "    samples = sfdpiv.index\n",
    "    asvs = sfdpiv.columns\n",
    "    \n",
    "    #Creating the dataframe with the clr transformed data, and assigning the sample names\n",
    "    clr_transformed = pd.DataFrame(clr_transformed_array, columns=asvs)\n",
    "    #Assigning the asv names\n",
    "    clr_transformed['samples'] = samples\n",
    "    clr_transformed = clr_transformed.set_index('samples')\n",
    "    clr_transformed.head()\n",
    "\n",
    "    #calculate distance matrix\n",
    "    dist = cdist(clr_transformed, clr_transformed, 'euclid')\n",
    "    distance_matrix = pd.DataFrame(dist, columns=samples)\n",
    "    distance_matrix['samples'] = samples\n",
    "    distance_matrix = distance_matrix.set_index('samples')\n",
    "\n",
    "    #format for pca\n",
    "    dm = DistanceMatrix(distance_matrix)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(distance_matrix)\n",
    "    \n",
    "    ####\n",
    "    sns.set(rc={\"figure.figsize\":(4, 3)})\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    plot_df = pd.DataFrame(data = pca_features, columns = ['dim1', 'dim2'], index = sfdpiv.index)\n",
    "    plot_df['dim1'] = plot_df['dim1']/1000\n",
    "    plot_df['dim2'] = plot_df['dim2']/1000\n",
    "    if depth =='all':\n",
    "        plot_df2 = pd.merge(plot_df,df[['sampleid','size_code','depth']],on='sampleid', how='left')\n",
    "    else:\n",
    "        plot_df2 = pd.merge(plot_df,df[['sampleid','size_code','weekn']],on='sampleid', how='left')\n",
    "        \n",
    "    \n",
    "    ##divide into pre-post bloom\n",
    "    def get_stage(weekNb):\n",
    "        if weekNb < 10:\n",
    "            return 'Pre-bloom'\n",
    "        elif weekNb == 10 :\n",
    "            return 'Bloom'\n",
    "        elif weekNb > 10:\n",
    "            return 'Bloom'\n",
    "    \n",
    "    if depth != 'all':\n",
    "        plot_df2['Time'] = plot_df2['weekn'].apply(get_stage)\n",
    "    \n",
    "    plot_df2 = plot_df2.rename(columns={'size_code': 'Size code'})\n",
    "    \n",
    "    pc1v = round(pca.explained_variance_ratio_[0]*100)\n",
    "    pc2v = round(pca.explained_variance_ratio_[1]*100)\n",
    "    \n",
    "    #plot_df2 = plot_df2.drop_duplicates()\n",
    "    #dfperm = plot_df2.set_index('sampleid')\n",
    "    \n",
    "    #permanova2 = permanova(dm, dfperm, columnperm)\n",
    "    #results = permanova2(999)\n",
    "    \n",
    "    #plot\n",
    "    \n",
    "    if depth == 'all':\n",
    "        var2 = 'depth'\n",
    "    else:\n",
    "        var2 = 'Time'\n",
    "    \n",
    "    sns.set_style(\"white\")\n",
    "    ax=sns.scatterplot(x = 'dim1', y = 'dim2', hue= 'Size code', style=var2, data = plot_df2, \n",
    "                       palette=palette_dict)#, size = 'Week_Group')#,palette=sns.color_palette(\"dark:salmon_r\", as_cmap=True))\n",
    "    plt.ylabel('PCo2 (' + str(pc2v) + '% variance explained)')\n",
    "    plt.xlabel('PCo1 (' + str(pc1v) +'% variance explained)')\n",
    "    ax.set_title('Depth ' + str(depth) + 'm', loc='left', weight='bold')\n",
    "    plt.legend(frameon=False)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    sns.despine()\n",
    "    plt.savefig('outputs/'+folder+'/D'+str(depth)+spc+'_PCAplot.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    print ( \"Components = \", pca.n_components_ , \";\\nTotal explained variance = \",\n",
    "      round(pca.explained_variance_ratio_.sum(),5)  )\n",
    "    \n",
    "    print (\"Components 1 and 2 are\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Retrieve Loadings\n",
    "    loadings = pca.components_\n",
    "\n",
    "    # Summarize Loadings by Metadata Category\n",
    "    metadata_groups = plot_df2[var2].unique()\n",
    "    metadata_contributions = {}\n",
    "    \n",
    "    for group in metadata_groups:\n",
    "        group_variables = plot_df2.loc[plot_df2[var2] == group, 'sampleid']\n",
    "        group_loadings = np.abs(loadings[:, [list(distance_matrix.columns).index(var) for var in group_variables]]).mean(axis=1)\n",
    "        metadata_contributions[group] = group_loadings\n",
    "\n",
    "    # Visual Representation\n",
    "    for group, contributions in metadata_contributions.items():\n",
    "        plt.barh(contributions, group) #range(1, len(contributions) + 1),\n",
    "\n",
    "    plt.ylabel('Principal Component')\n",
    "    plt.xlabel('Average Loading Contribution')\n",
    "    sns.despine()\n",
    "    plt.legend(frameon=False)\n",
    "    plt.savefig('outputs/'+folder+'/D'+str(depth)+spc+'_PCAplot_brplot.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "        \n",
    "\n",
    "    ##clustermap\n",
    "    ax = sns.clustermap(distance_matrix, method=\"complete\", cmap='RdBu', annot=True,\n",
    "               yticklabels=True, row_colors = dfcolors,\n",
    "               annot_kws={\"size\": 7}, figsize=(15,12));\n",
    "\n",
    "    handles1 = [Patch(facecolor=palette_dict_month[key]) for key in palette_dict_month]\n",
    "    plt.legend(handles1, palette_dict_month, title='Month',\n",
    "               bbox_to_anchor=(1, 1), bbox_transform=plt.gcf().transFigure, loc='upper left')\n",
    "    \n",
    "    plt.savefig('outputs/'+folder+'/D'+str(depth)+spc+'_clustermap.png', dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "    return pca, pca_features, sfdclr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_depth(separated, comm, depth, ycolumn, yaxislabel='def'):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "    \n",
    "    if yaxislabel != 'def':\n",
    "        ycol = ycolumn\n",
    "    \n",
    "    #sfd=separated[separated.depth==depth]\n",
    "    sfd = separated.copy()\n",
    "    \n",
    "    #sfd_S = sfd[['size_code', 'nASVs', 'weekn']].copy()\n",
    "    #sfd_S = sfd_S.drop_duplicates()\n",
    "    #sdfpv = sfd_S.pivot(index='weekn', columns='size_code', values='nASVs')\n",
    "    #fvalue, pvalue = stats.f_oneway(sdfpv['L'], sdfpv['S'], sdfpv['W'])\n",
    "    \n",
    "    sfd_LM = sfd[['size_code', 'nASVs']].copy()\n",
    "    sfd_LM = sfd_LM.drop_duplicates()\n",
    "    lm = sfa.ols('nASVs ~ C(size_code)', data=sfd_LM).fit()\n",
    "    anova = sa.stats.anova_lm(lm)\n",
    "    results = spPH.posthoc_ttest(sfd_LM, val_col='nASVs', group_col='size_code', p_adjust='holm')\n",
    "    \n",
    "    if 'SL' in separated['size_code'].unique():\n",
    "        #sizecode palette codes\n",
    "        sizecodes = ['S', 'L', 'W', 'SL']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "    \n",
    "    else:\n",
    "        #define color palettes\n",
    "        sizecodes = ['S', 'L', 'W']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plot\n",
    "    sns.set(rc={\"figure.figsize\":(4, 3)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.boxplot(data=sfd, x=\"size_code\", y=ycolumn, palette=palette_dict, order=sizecodes)#, hue=\"size_code\")\n",
    "    sns.despine()\n",
    "    plt.ylabel(yaxislabel, fontsize=20)\n",
    "    plt.xlabel('Size fraction', fontsize=20)\n",
    "\n",
    "    #g.tick_params(labelsize=15)\n",
    "    plt.savefig('outputs/'+comm_id+'/D'+str(depth)+'_adboxplot.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    sns.set(rc={\"figure.figsize\":(7, 3)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    ax=sns.barplot(data=sfd, x=\"weekn\", y=\"diff\", hue=\"size_code\", palette=palette_dict,\n",
    "                  capsize=.15, errwidth=0.5)#, hue=\"size_code\")\n",
    "    sns.despine()\n",
    "    plt.ylabel('Number of ASVs relative to weekly average', fontsize=20)\n",
    "    plt.xlabel('Week number', fontsize=20)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.savefig('outputs/'+comm_id+'/D'+str(depth)+'_avgbarplot.png', dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.clf() \n",
    "    \n",
    "\n",
    "#     glue = sfd[['size_code', 'weekn', 'diff']].copy()\n",
    "#     glue = glue.drop_duplicates()\n",
    "#     glue = glue.pivot(index=\"size_code\", columns=\"weekn\", values=\"diff\")\n",
    "#     floored_data = glue.apply(np.floor)\n",
    "#     sns.set_style('ticks')\n",
    "#     plt.figure(figsize=(8, 2))\n",
    "#     cmap = sns.diverging_palette(240,240, as_cmap=True)\n",
    "#     ax = sns.heatmap(floored_data, yticklabels=True, linewidths=.5, annot=True, annot_kws={\"fontsize\":8},\n",
    "#                     cmap = cmap)\n",
    "#     plt.savefig('outputs/'+comm_id+'/heatmap_nasv_change_d'+str(depth)+'_annot.png', bbox_inches='tight', dpi=300)\n",
    "#     plt.clf() \n",
    "    \n",
    "#     ax = sns.heatmap(floored_data, fmt='.1f', yticklabels=True, linewidths=.5,\n",
    "#                     cmap = cmap)\n",
    "#     plt.savefig('outputs/'+comm_id+'/heatmap_nasv_change_d'+str(depth)+'.png', bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    \n",
    "    plt.clf() \n",
    "    sns.set(rc={\"figure.figsize\":(7, 3)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    ax=sns.lineplot(x = \"weekn\", y = ycolumn, data=sfd, hue=\"size_code\", palette=palette_dict)\n",
    "    sns.despine()\n",
    "    plt.ylabel(yaxislabel, fontsize=20)\n",
    "    plt.xlabel('Week', fontsize=20)\n",
    "    plt.legend(title='Size fraction')\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.savefig('outputs/'+comm_id+'/D'+str(depth)+'_adlineplot.png', dpi=200, bbox_inches=\"tight\")\n",
    "    \n",
    "    return anova, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsetprep(comm, level, separated):\n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    cumulab = separated[['feature_frequency', 'depth', 'size_code', level]].copy()\n",
    "    cumulab1 = cumulab.groupby([level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "    for d in depths:\n",
    "        #make csv\n",
    "        sfd=separated[separated.depth==d]\n",
    "        \n",
    "        toptaxa = sfd[['feature_id', 'feature_frequency', 'Taxon', 'size_code', 'depth','weekn', level]].copy()\n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level, 'depth']).agg({'feature_frequency':sum})\n",
    "        topd = df_agg['feature_frequency'].groupby('size_code', group_keys=False).nlargest(10)\n",
    "        topd = topd.to_frame()\n",
    "        topd = topd.reset_index()\n",
    "\n",
    "        df_agg = df_agg.reset_index()\n",
    "        df_agg['set_name'] = df_agg['size_code']+df_agg['depth'].astype(str)\n",
    "    \n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='set_name', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "        resultpivot[resultpivot != 0] = 1\n",
    "        tosave = pd.merge(resultpivot, cumulab1, left_index=True, right_index=True)\n",
    "        tosave.to_csv('csvs/'+comm_id+'/'+level+'_d'+str(d)+'_relab.csv')\n",
    "        \n",
    "        \n",
    "        #make json\n",
    "        data = {\n",
    "            \"file\": \"https://raw.githubusercontent.com/dianahaider/size_fractions/main/csvs/\"+comm_id+'/'+level+'_d'+str(d)+'_relab.csv',\n",
    "            \"name\": comm + level,\n",
    "            \"header\": 0,\n",
    "            \"separator\": \",\",\n",
    "            \"skip\": 0,\n",
    "            \"meta\":[\n",
    "                {\"type\":\"id\", \"index\":0, \"name\":\"Name\"},\n",
    "                {\"type\":\"integer\", \"index\":4, \"name\":\"Rel. ab.\"}\n",
    "            ],\n",
    "            \"sets\": [\n",
    "                {\"format\": \"binary\", \"start\":1, \"end\": 3}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open('json/'+comm_id+'/'+level+'_d'+str(d)+'.json', 'w') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_fid(comm, separated, depth, fid):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "    \n",
    "    if 'SL' in separated['size_code'].unique():\n",
    "        #sizecode palette codes\n",
    "        sizecodes = ['S', 'L', 'W', 'SL']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "    \n",
    "    else:\n",
    "        #sizecode palette codes\n",
    "        sizecodes = ['S', 'L', 'W']\n",
    "        palette_colors = sns.color_palette()\n",
    "        palette_dict = {sizecode: color for sizecode, color in zip(sizecodes, palette_colors)}\n",
    "    \n",
    "    sfd=separated[separated.depth==depth]\n",
    "    sfd['weekfid'] = sfd[\"weekn\"].astype(str) + sfd[\"feature_id\"].astype(str)\n",
    "    sfd['avg_p_id'] = sfd['ratio'].groupby(sfd['weekfid']).transform('mean')\n",
    "    sfd['diff_p_id'] = sfd['ratio'] - sfd['avg_p_id']\n",
    "    \n",
    "    sfd_f=sfd[sfd.feature_id==fid]\n",
    "    \n",
    "    ttl = sfd_f['Taxon'].iloc[0]\n",
    "    \n",
    "    sns.set(rc={\"figure.figsize\":(7, 3)})\n",
    "    ax=sns.barplot(data=sfd_f, x=\"weekn\", y=\"diff_p_id\", hue=\"size_code\", palette=palette_dict)#, hue=\"size_code\")\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.title(ttl)\n",
    "    plt.ylabel('Ratio difference')\n",
    "    plt.xlabel('Week number')\n",
    "    plt.savefig('outputs/'+comm_id+'/D'+str(depth)+fid+'.png', dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ancom(separated, sfdclr, depth, ancomcol):\n",
    "    \n",
    "    sfd=separated[separated.depth==depth]\n",
    "\n",
    "        \n",
    "    df_ancom = sfd[['sampleid', ancomcol]].copy()\n",
    "    df_ancom = df_ancom.drop_duplicates()\n",
    "    df_ancom = df_ancom.set_index('sampleid')\n",
    "    \n",
    "    results = ancom(table=sfdclr, grouping=df_ancom[ancomcol])\n",
    "    \n",
    "    DAresults = results[0].copy()\n",
    "    DARejected_SC = DAresults.loc[DAresults['Reject null hypothesis'] == True]\n",
    "    DARejected_SC.sort_values(by=['W'])\n",
    "    \n",
    "    taxonomy = sfd[['feature_id', 'Confidence', 'Taxon', 'Phylum', 'Class', 'Family', 'Genus', 'Species']].copy()\n",
    "    taxonomy = taxonomy.drop_duplicates()\n",
    "    DARejected_SC_taxonomy = pd.merge(DARejected_SC, taxonomy, on=\"feature_id\", how=\"left\")\n",
    "    DARejected_SC_taxonomy.sort_values(by='W')\n",
    "    \n",
    "    prcentile = results[1].copy()\n",
    "    \n",
    "    return DARejected_SC_taxonomy, prcentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = 'From Jan7 2022 to Apr27 2022'\n",
    "def plot_stackedbar_(df, labels, colors, title, subtitle, level):\n",
    "        \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    fields = df.columns.tolist()\n",
    "    \n",
    "    # figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "# plot bars\n",
    "    left = len(df) * [0]\n",
    "    for idx, name in enumerate(fields):\n",
    "        plt.barh(df.index, df[name], left = left, color=colors[idx])\n",
    "        left = left + df[name]\n",
    "# title and subtitle\n",
    "    plt.title(title, loc='left')\n",
    "    #plt.text(0, ax.get_yticks()[-1] + 0.75, subtitle)\n",
    "# legend\n",
    "    plt.legend(labels, bbox_to_anchor=([1, 1, 0, 0]), ncol=1, frameon=False)\n",
    "# remove spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "# format x ticks\n",
    "    xticks = np.arange(0,110,10)\n",
    "    xlabels = ['{}%'.format(i) for i in np.arange(0,101,10)]\n",
    "    plt.xticks(xticks, xlabels)\n",
    "# adjust limits and draw grid lines\n",
    "    plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "    ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.savefig('outputs/'+comm_id+'/'+level+'alldepths_stacked_perc_weighted.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = 'From Jan7 2022 to Apr27 2022'\n",
    "def plot_stackedbar_p(df, labels, colors, title, subtitle, level):\n",
    "        \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    fields = df.columns.tolist()\n",
    "    \n",
    "    # figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "# plot bars\n",
    "    left = len(df) * [0]\n",
    "    for idx, name in enumerate(fields):\n",
    "        plt.barh(df.index, df[name], left = left, color=colors[idx])\n",
    "        left = left + df[name]\n",
    "# title and subtitle\n",
    "    plt.title(title, loc='left')\n",
    "    #plt.text(0, ax.get_yticks()[-1] + 0.75, subtitle)\n",
    "# legend\n",
    "    plt.legend(labels, bbox_to_anchor=([1, 1, 0, 0]), ncol=1, frameon=False)\n",
    "# remove spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "# format x ticks\n",
    "    xticks = np.arange(0,110,10)\n",
    "    xlabels = ['{}%'.format(i) for i in np.arange(0,101,10)]\n",
    "    plt.xticks(xticks, xlabels)\n",
    "# adjust limits and draw grid lines\n",
    "    plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "    ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.savefig('outputs/'+comm_id+'/'+level+'alldepths_stacked_perc_weighted.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = 'From Jan7 2022 to Apr27 2022'\n",
    "def plot_stackedbar_p(df, labels, colors, title, subtitle, level):\n",
    "        \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    fields = df.columns.tolist()\n",
    "    \n",
    "    # figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "# plot bars\n",
    "    left = len(df) * [0]\n",
    "    for idx, name in enumerate(fields):\n",
    "        plt.barh(df.index, df[name], left = left, color=colors[idx])\n",
    "        left = left + df[name]\n",
    "# title and subtitle\n",
    "    plt.title(title, loc='left')\n",
    "    #plt.text(0, ax.get_yticks()[-1] + 0.75, subtitle)\n",
    "# legend\n",
    "    plt.legend(labels, bbox_to_anchor=([1, 1, 0, 0]), ncol=1, frameon=False)\n",
    "# remove spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "# format x ticks\n",
    "    xticks = np.arange(0,110,10)\n",
    "    xlabels = ['{}%'.format(i) for i in np.arange(0,101,10)]\n",
    "    plt.xticks(xticks, xlabels)\n",
    "# adjust limits and draw grid lines\n",
    "    plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "    ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.savefig('outputs/'+comm_id+'/'+level+'alldepths_stacked_perc_weighted.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc_defrac(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'SF', 'NSF', 'Both', 'DFr'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df1 = resultpivot.copy()\n",
    "    \n",
    "        df = resultpivot[['L', 'S', 'W']].copy()\n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "        \n",
    "        DFr = df1[(df1['SL'] != 0)]\n",
    "        DFr = DFr[['SL']].copy()\n",
    "    \n",
    "        total = resultpivot.to_numpy().sum()\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = SF.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = Both.to_numpy().sum()/total *100\n",
    "    \n",
    "        Wonly_value = Wonly.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "        Lonly_value = Lonly.to_numpy().sum()/total *100\n",
    "        LS_value = LS.to_numpy().sum()/total *100\n",
    "        DFr_value = DFr.to_numpy().sum()/total *100\n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'SF'] = SF_value\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot.loc[d,'Both'] = Both_value\n",
    "        dfplot.loc[d,'DFr'] = DFr_value\n",
    "        \n",
    "        dfplot_unweighted.loc[d,'Depth'] = depths[d]\n",
    "        dfplot_unweighted.loc[d,'SF'] = len(Lonly) + len(Sonly) + len(LS)\n",
    "        dfplot_unweighted.loc[d,'NSF'] = len(Wonly)\n",
    "        dfplot_unweighted.loc[d,'Both'] = len(LW) + len(SW) + len(LSW)\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "        \n",
    "    return dfplot, level, dfplot_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc_defrac_unweighted(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'SF', 'NSF', 'Both'])\n",
    "    dfplot_unweighted = pd.DataFrame(columns=['Depth', 'SF', 'NSF', 'Both'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df1 = resultpivot.copy()\n",
    "    \n",
    "        df = resultpivot[['L', 'S', 'W']].copy()\n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "    \n",
    "        total = len(resultpivot)\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = len(SF)/total *100\n",
    "        \n",
    "        Sonly_value = len(Sonly)/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = len(Both)/total *100\n",
    "    \n",
    "        Wonly_value = len(Wonly)/total *100\n",
    "        \n",
    "        Sonly_value = len(Sonly)/total *100\n",
    "        Lonly_value = len(Lonly)/total *100\n",
    "        LS_value = len(LS)/total *100\n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'SF'] = SF_value\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot.loc[d,'Both'] = Both_value\n",
    "        \n",
    "        dfplot_unweighted.loc[d,'Depth'] = depths[d]\n",
    "        dfplot_unweighted.loc[d,'SF'] = Lonly_value + Sonly_value + LS_value\n",
    "        dfplot_unweighted.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot_unweighted.loc[d,'Both'] = 100 - (Lonly_value + Sonly_value + LS_value + Wonly_value)\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "        \n",
    "    return dfplot, level, dfplot_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'SF', 'NSF', 'Both'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df = resultpivot.copy()\n",
    "    \n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "    \n",
    "        total = df.to_numpy().sum()\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = SF.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = Both.to_numpy().sum()/total *100\n",
    "    \n",
    "        Wonly_value = Wonly.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "        Lonly_value = Lonly.to_numpy().sum()/total *100\n",
    "        LS_value = LS.to_numpy().sum()/total *100\n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'SF'] = SF_value\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot.loc[d,'Both'] = Both_value\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "        \n",
    "    return dfplot, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc_SLNSF(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'Sonly', 'Lonly', 'LS', 'NSF'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df = resultpivot.copy()\n",
    "    \n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "    \n",
    "        total = df.to_numpy().sum()\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = SF.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = Both.to_numpy().sum()/total *100\n",
    "    \n",
    "        Wonly_value = Wonly.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "        Lonly_value = Lonly.to_numpy().sum()/total *100\n",
    "        LS_value = LS.to_numpy().sum()/total *100\n",
    "        \n",
    "        NewTotal = Sonly_value + Lonly_value + LS_value + Wonly_value\n",
    "        \n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'Sonly'] = Sonly_value\n",
    "        dfplot.loc[d,'Lonly'] = Lonly_value\n",
    "        dfplot.loc[d,'LS'] = LS_value\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "    dfplot_normalized = dfplot/NewTotal *100\n",
    "        \n",
    "    return dfplot, dfplot_normalized, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc_LSW(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'NSF', 'LW', 'SW', 'LSW'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df = resultpivot.copy()\n",
    "    \n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "    \n",
    "        total = df.to_numpy().sum()\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = SF.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = Both.to_numpy().sum()/total *100\n",
    "    \n",
    "        Wonly_value = Wonly.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "        Lonly_value = Lonly.to_numpy().sum()/total *100\n",
    "        LS_value = LS.to_numpy().sum()/total *100\n",
    "        \n",
    "        LW_value = LW.to_numpy().sum()/total *100\n",
    "        SW_value = SW.to_numpy().sum()/total *100\n",
    "        LSW_value = LSW.to_numpy().sum()/total *100\n",
    "        \n",
    "        NewTotal = Sonly_value + Lonly_value + LS_value + Wonly_value\n",
    "        \n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot.loc[d,'LW'] = LW_value\n",
    "        dfplot.loc[d,'SW'] = SW_value\n",
    "        dfplot.loc[d,'LSW'] = LSW_value\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "    dfplot_normalized = dfplot/NewTotal *100\n",
    "        \n",
    "    return dfplot, dfplot_normalized, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcperc_LS_W(comm, separated, level):\n",
    "    \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    depths = [1, 5, 10, 30, 60]\n",
    "    \n",
    "    level = level\n",
    "    \n",
    "    dfplot = pd.DataFrame(columns=['Depth', 'NSF', 'LW', 'SW'])\n",
    "    \n",
    "    for d in range(len(depths)):\n",
    "        sfd=separated[separated.depth==depths[d]]\n",
    "        toptaxa = sfd[[level, 'feature_frequency', 'Taxon', 'size_code', 'weekn']].copy()\n",
    "    \n",
    "        toptaxa = toptaxa.drop_duplicates()\n",
    "        df_agg = toptaxa.groupby(['size_code',level]).agg({'feature_frequency':sum})\n",
    "    \n",
    "        df_agg = df_agg.reset_index()\n",
    "        resultpivot = df_agg.pivot_table(index=level, columns='size_code', values='feature_frequency')\n",
    "        resultpivot = resultpivot.fillna(0)\n",
    "    \n",
    "        df = resultpivot.copy()\n",
    "    \n",
    "        Sonly = df[(df['L'] == 0) & (df['W'] == 0)]\n",
    "        Wonly = df[(df['L'] == 0) & (df['S'] == 0)]\n",
    "        Lonly = df[(df['S'] == 0) & (df['W'] == 0)]\n",
    "        LW = df[(df['S'] == 0) & (df['W'] != 0) & (df['L'] != 0)]\n",
    "        LS = df[(df['W'] == 0) & (df['S'] != 0) & (df['L'] != 0)]\n",
    "        SW = df[(df['W'] != 0) & (df['S'] != 0) & (df['L'] == 0)]\n",
    "        LSW = df[~(df == 0).any(axis=1)]\n",
    "    \n",
    "        total = df.to_numpy().sum()\n",
    "    \n",
    "        SFdf = Lonly, LS, Sonly\n",
    "        SF = pd.concat(SFdf)\n",
    "        SF_value = SF.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "    \n",
    "    \n",
    "        Bothdf = LW, LSW, SW\n",
    "        Both = pd.concat(Bothdf)\n",
    "        Both_value = Both.to_numpy().sum()/total *100\n",
    "    \n",
    "        Wonly_value = Wonly.to_numpy().sum()/total *100\n",
    "        \n",
    "        Sonly_value = Sonly.to_numpy().sum()/total *100\n",
    "        Lonly_value = Lonly.to_numpy().sum()/total *100\n",
    "        LS_value = LS.to_numpy().sum()/total *100\n",
    "        \n",
    "        LW_value = LW.to_numpy().sum()/total *100\n",
    "        SW_value = SW.to_numpy().sum()/total *100\n",
    "        LSW_value = LSW.to_numpy().sum()/total *100\n",
    "        \n",
    "        NewTotal = Sonly_value + Lonly_value + LS_value + Wonly_value\n",
    "        \n",
    "        \n",
    "        dfplot.loc[d,'Depth'] = depths[d]\n",
    "        dfplot.loc[d,'NSF'] = Wonly_value\n",
    "        dfplot.loc[d,'LW'] = LW_value\n",
    "        dfplot.loc[d,'SW'] = SW_value\n",
    "        \n",
    "\n",
    "        venn3(subsets = (len(Lonly), len(Sonly), len(LS), len(Wonly), len(LW), len(SW), len(LSW)), set_labels = ('Large >3μm', 'Small 3-02μm', 'Whole water <0.22μm'), alpha = 0.5);\n",
    "\n",
    "        plt.savefig(\"outputs/\"+comm_id+\"/D\"+str(depths[d])+level+\"_venn.png\")\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    dfplot['Depth'] = dfplot['Depth'].astype(str)\n",
    "    dfplot = dfplot.set_index('Depth')\n",
    "    dfplot_normalized = dfplot/NewTotal *100\n",
    "        \n",
    "    return dfplot, dfplot_normalized, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtitle = 'From Jan7 2022 to Apr27 2022'\n",
    "def plot_stackedbar_p_SLNSF(df, labels, colors, title, subtitle, level, xmax=110, xtick=10):\n",
    "        \n",
    "    if comm == '16S':\n",
    "        comm_id = '02-PROKs'\n",
    "    else:\n",
    "        comm_id = '02-EUKs'\n",
    "        \n",
    "    fields = df.columns.tolist()\n",
    "    \n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    \n",
    "    # figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 5))\n",
    "# plot bars\n",
    "    left = len(df) * [0]\n",
    "    for idx, name in enumerate(fields):\n",
    "        plt.barh(df.index, df[name], left = left, color=colors[idx])\n",
    "        left = left + df[name]\n",
    "# title and subtitle\n",
    "    plt.title(title, loc='left')\n",
    "    #plt.text(0, ax.get_yticks()[-1] + 0.75, subtitle)\n",
    "# legend\n",
    "    plt.legend(labels, bbox_to_anchor=([1, 1, 0, 0]), ncol=1, frameon=False)\n",
    "# remove spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "# format x ticks\n",
    "    xticks = np.arange(0,xmax,xtick)\n",
    "    xlabels = ['{}%'.format(i) for i in np.arange(0,xmax,xtick)]\n",
    "    plt.xticks(xticks, xlabels)\n",
    "# adjust limits and draw grid lines\n",
    "    plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "    ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.ylabel(\"Depth (m)\")\n",
    "    plt.savefig('outputs/'+comm_id+'/'+level+'alldepths_stacked_perc_weighted'+name+'.png', dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-2023.5",
   "language": "python",
   "name": "qiime2-2023.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
